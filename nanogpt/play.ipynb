{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "import random\n",
    "\n",
    "WORD_LEN = 8\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = 65  # Where did 32 come from for C?\n",
    "# N_EMBD = 32\n",
    "NUM_STEPS = 1000\n",
    "VALIDATION_FRACTION = 0.8\n",
    "\n",
    "with open('input.txt', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "def pick_random_batches(contents, vocab_dict, validation = False):\n",
    "    input = torch.zeros((BATCH_SIZE, WORD_LEN), dtype=torch.int64)\n",
    "    target = torch.zeros((BATCH_SIZE, WORD_LEN), dtype=torch.int64)\n",
    "    for i in range(BATCH_SIZE):\n",
    "        start = random.randint(0, int((len(contents) - WORD_LEN - 1) * VALIDATION_FRACTION))\n",
    "        if validation:\n",
    "            start = random.randint(int(len(contents) * VALIDATION_FRACTION), (len(contents) - WORD_LEN - 1))\n",
    "        input[i] = torch.tensor([vocab_dict[c]\n",
    "                              for c in contents[start:start+WORD_LEN]])\n",
    "        target[i] = torch.tensor([vocab_dict[c]\n",
    "                              for c in contents[start + 1:start + 1 + WORD_LEN]])\n",
    "    return (input, target)\n",
    "    # input = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7]).repeat(BATCH_SIZE, 1)\n",
    "    # target = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).repeat(BATCH_SIZE, 1)\n",
    "    # return (input, target)\n",
    "\n",
    "class ValidDataLoaderIterator:\n",
    "    def __init__(self):\n",
    "        self.idx = int(len(contents) * VALIDATION_FRACTION)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx + 1 + WORD_LEN > len(contents):\n",
    "            raise StopIteration\n",
    "\n",
    "        input = torch.zeros((BATCH_SIZE, WORD_LEN), dtype=torch.int64)\n",
    "        target = torch.zeros((BATCH_SIZE, WORD_LEN), dtype=torch.int64)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            input[i] = torch.tensor([vocab_dict[c]\n",
    "                                for c in contents[self.idx:self.idx+WORD_LEN]])\n",
    "            target[i] = torch.tensor([vocab_dict[c]\n",
    "                                for c in contents[self.idx + 1:self.idx + 1 + WORD_LEN]])\n",
    "            self.idx += WORD_LEN\n",
    "        return (input, target)\n",
    "\n",
    "class ValidDataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return ValidDataLoaderIterator()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_vocab(contents):\n",
    "    vocab_dict = {}\n",
    "\n",
    "    letters = collections.defaultdict(int)\n",
    "    for c in contents:\n",
    "        letters[c] += 1\n",
    "    keys = sorted(letters.keys())\n",
    "    count = 0\n",
    "    for k in keys:\n",
    "        # print('fill', k, count)\n",
    "        vocab_dict[k] = count\n",
    "        count += 1\n",
    "    # print(vocab_dict)\n",
    "    # print(len(keys))\n",
    "    return vocab_dict\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(VOCAB_SIZE, VOCAB_SIZE)\n",
    "        # self.sm = nn.Softmax(dim=2)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # input is (B, T)\n",
    "        # output is (B, T, C)\n",
    "        logits = self.embedding(input)\n",
    "        loss = self.loss_fn(torch.permute(logits, (0, 2, 1)), target)\n",
    "        # print('out sum', out[0, 0].sum())\n",
    "        return logits, loss\n",
    "\n",
    "# def loss_fn(output, target):\n",
    "#     target_one_hot = F.one_hot(target, num_classes=VOCAB_SIZE)\n",
    "#     # print(output.shape, target.shape, target_one_hot.shape)\n",
    "#     # print((output - target_one_hot)[0,0])\n",
    "#     # print(output.sum(dim=2))\n",
    "#     # print('sums', ((output - target_one_hot) ** 2).sum(dim=2))\n",
    "#     return ((output - target_one_hot) ** 2).sum()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "vocab_dict = get_vocab(contents)\n",
    "\n",
    "model = BigramModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss = None\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "for i in range(NUM_STEPS):\n",
    "    train_input, train_target = pick_random_batches(contents, vocab_dict, validation = False)\n",
    "    # print(input.dtype)\n",
    "    optimizer.zero_grad()\n",
    "    _output, loss = model(train_input, train_target)\n",
    "    # print(output.shape)\n",
    "    # loss = loss_fn(output, target)\n",
    "\n",
    "\n",
    "    # print(model.parameters())\n",
    "    loss.backward()\n",
    "    # for param in model.parameters():\n",
    "    #     print('grad shape', param.grad.shape)\n",
    "    #     print('add grad', param.grad)\n",
    "    optimizer.step()\n",
    "    \n",
    "    valid_input, valid_target = pick_random_batches(contents, vocab_dict, validation = True)\n",
    "    with torch.no_grad():\n",
    "        _output, valid_loss = model(valid_input, valid_target)\n",
    "    train_losses.append(loss.item())\n",
    "    valid_losses.append(valid_loss.item())\n",
    "print('train loss', loss)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('default')\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(valid_losses, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
